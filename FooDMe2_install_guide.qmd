---
title: "FooDMe2 installation guide"
subtitle: "A step-by-step guide for FooDMe2 v1.1.0"
author:
  - name: G. Denay
    affiliation: CVUA-RRW
  - name: M. Hoeppner
    affiliation: LSH
date: now
date-format: DD.MM.YYYY
title-block-banner: "#3F51B5"
title-block-banner-color: white
format:
  html:
    theme: lumen
    embed-resources: true
    anchor-sections: true
    smooth-scroll: true
    link-external-icon: true
    link-external-newwindow: true
    link-external-filter: ^(?:http:|https:)\/\/(bio-raum\.github\.io|github\.com\/bio-raum)\/FooDMe2
    code-copy: true
    toc: true
    toc-title: Quickjump
    toc-location: left
    other-links:
    - text: Online documentation
      icon: lightbulb
      href: https://bio-raum.github.io/FooDMe2/latest/
    - text: Ask a question
      icon: question-lg
      href: https://github.com/bio-raum/FooDMe2/issues/new?template=QUESTION.yml
    - text: Report a problem
      icon: bug-fill
      href: https://github.com/bio-raum/FooDMe2/issues/new?template=BUG-REPORT.yml
execute:
  echo: true
  eval: false
code-annotations: hover
website:
  site-url: https://github.com/bio-raum/FooDMe2
---

<!--
to render this document use:
```
quarto install extension shafayetShafee/bsicons
quarto render <filename>
```
-->

This document aims to be a step-by-step handy guide to your first installation, configuration, and usage of FooDMe2.

You can copy the code from each block and paste it in your shell to follow along. Hover over the circled numbers in the
code blocks to know more.

## Requirements

To follow along you will need:

- A UNIX system: any Linux distribution, MacOS, or Windows Subsystem for Linux.
- An Internet connection. If nescessary, the proxy settings.
- For the installation of Nextflow (compulsary) and Docker or Singularity/Apptainer (optional), you will need root (`sudo`) access.

We assume basic knowledge of the UNIX shell and the examples used will rely solely on the analysis of paired-end illumina data of meat (mammals and birds) samples.

## Installation

### Installing Nextflow

The first step is to install the workflow manager Nextflow, which is going to take care of the workflow execution planning.

First check wether nextflow is installed on your system:

```bash
nextflow info
```

If the nextflow version is 23 or more, you are good to go!

If nextflow is installed with an older version run:

```bash
nextflow self-update
```

If nextflow is not installed, you will need to follow the [step-by-step instructions from the Nextflow documentation](https://www.nextflow.io/docs/latest/install.html#requirements)

### Dependency managers

Nexflow relies on dependency managers for the deployement of software dependencies in self contained environments or containers.
We always recommend that you run FooDMe2 with a container engine, such as [Docker](https://www.docker.com/), [Apptainer](https://apptainer.org/) or [Singularity](https://docs.sylabs.io/guides/latest/user-guide/index.html). Containers are faster to install and a more reliable way to ensure pipeline reproducibility over time. If containers
are not an option in your setting, you can also use the Conda/Mamba package manager. 

::: {.panel-tabset group="depmanager"}

## Apptainer

Check if installed:

```bash
apptainer -help
```

If it is not installed, check the [official installation guide](https://apptainer.org/docs/admin/latest/installation.html#install-from-pre-built-packages), or choose another manager.

## Singularity

Check if installed:

```bash
singularity -help
```

If it is not installed, check the [official installation guide](https://docs.sylabs.io/guides/latest/user-guide/quick_start.html#download-singularityce-from-a-release), or choose another manager.

## Docker

Check if installed:

```bash
docker -help
```

Ensure that your user is part of the Docker group ('docker'), or that Docker is otherwise configured to allow you to execute it.

If it is not installed, check the [official installation guide](https://docs.docker.com/engine/install/), or choose another manager.

## Conda

Check if installed:

```bash
conda info
```

If it is, great! Make sure it is configured properly to find and installe the packages:

```bash
conda config --add channels bioconda
conda config --add channels conda-forge
conda config --set channel_priority strict
```

If it is not installed, we recommend you to install the miniforge distribution following the [official guide](https://github.com/conda-forge/miniforge?tab=readme-ov-file#unix-like-platforms-macos-linux--wsl).
When you are done, configure the channels as above. You will need to close you shell and reopen it after the installation for the `conda` command to be available.

:::

### Nextflow configuration

In order to set some of the execution options for nextflow, like ressource allocation, dependency manager, or where to look for the locally installed databases, it is possible to configure Nextflow using one of two options:

- remote configuration (recommended) on the central `bio-raum/nf-configs` [github repository](https://github.com/bio-raum/nf-configs/tree/main), allows to fully pre-configure FooDMe2 in a way that is specific to your setting with a single `switch`.
- alternatively, one can use a local configuration file, although this limits the scope of possible configurations.
- no configuration (not recommended) is also possible, but you will have to manually provide parameters at each execution.

A few examples are given below; these can be extended as described in the [Nextflow documentation](https://www.nextflow.io/docs/latest/config.html#configuration-file).

:::: {.panel-tabset group="config"}
## Remote

To upload your own configuration to the central `bio-raum/nf-configs` repository, [open an issue](https://github.com/bio-raum/nf-configs/issues/new) or directly send a pull-request. Alternatively, you can use on of the existing configuration if it fits your needs.

Before using the examples below, make sure you modify the ressources (`max_cpus` and `max_memory`) so that they fit your system, and adjust the paths that will be used to store the databases and dependencies on your system (`reference_base` and `cacheDir`).
If you run on a local system with a single user, you may install the references into your home directory ($HOME, /home/youruser). In a shared setting, particularly on distributed compute infrastructures, the references should be installed
to a shared directory where all users can access them. 

::: {.panel-tabset group="depmanager"}

### Apptainer

```{.js filename="remote.config"}
params {
	max_cpus = 10
	max_memory = 64.GB
	reference_base = "$HOME/nextflow/refs"
}

process {
	executor = 'local'
}

executor {
	queueSize=5
}

apptainer {
	enabled = true
	cacheDir = "$HOME/nextflow/envs"
}
```

### Singularity

```{.js filename="remote.config"}
params {
	max_cpus = 10
	max_memory = 64.GB
	reference_base = "$HOME/nextflow/refs"
}

process {
	executor = 'local'
}

executor {
	queueSize=5
}

singularity {
	enabled = true
	cacheDir = "$HOME/nextflow/envs"
}
```

### Docker

```{.js filename="remote.config"}
params {
	max_cpus = 10
	max_memory = 64.GB
	reference_base = "$HOME/nextflow/refs"
}

process {
	executor = 'local'
}

executor {
	queueSize=5
}

docker {
	enabled = true
	cacheDir = "$HOME/nextflow/envs"
}
```

### Conda

```{.js filename="remote.config"}
params {
	max_cpus = 10
	max_memory = 64.GB
	reference_base = "$HOME/nextflow/refs"
}

process {
	executor = 'local'
}

executor {
	queueSize=5
}

conda {
	enabled = true
	cacheDir = "$HOME/nextflow/envs"
	useMamba = true
}
```

:::

## Local

For the following it will be assumed that the configuration file is saved under `$HOME/nextflow` as `local.config`

::: {.panel-tabset group="depmanager"}

### Apptainer

```{.js filename="local.config"}
params {
	max_cpus = 10
	max_memory = 64.GB
}

process {
	executor = 'local'
}

executor {
	queueSize=5
}

apptainer {
	enabled = true
	cacheDir = "$HOME/nextflow/envs"
}
```

### Singularity

```{.js filename="local.config"}
params {
	max_cpus = 10
	max_memory = 64.GB
}

process {
	executor = 'local'
}

executor {
	queueSize=5
}

singularity {
	enabled = true
	cacheDir = "$HOME/nextflow/envs"
}
```

### Docker

```{.js filename="local.config"}
params {
	max_cpus = 10
	max_memory = 64.GB
}

process {
	executor = 'local'
}

executor {
	queueSize=5
}

docker {
	enabled = true
	cacheDir = "$HOME/nextflow/envs"
}
```

### Conda

```{.js filename="local.config"}
params {
	max_cpus = 10
	max_memory = 64.GB
}

process {
	executor = 'local'
}

executor {
	queueSize=5
}

conda {
	enabled = true
	cacheDir = "$HOME/nextflow/envs"
	useMamba = true
}
```

:::

::::

### Distributed compute infrastructures

All examples assume that you run FooDMe2 on a local system (executor = local). If you are planning to use this pipeline on a distributed compute system ("cluster"), or the cloud, please refer to the [Nextflow documentation](https://www.nextflow.io/docs/latest/executor.html) to learn about alternative execution profiles. 
An example for a Slurm cluster can be found [here](https://github.com/bio-raum/nf-configs/blob/main/conf/lsh.config).

### Installing the databases

Now that everything is installed and Nextflow is configured, we can install the databases that will be used for the analysis.

::: {.panel-tabset group="config"}

## Remote

```bash
nextflow run bio-raum/FooDMe2 \
  -profile remote \                         # <1>
  -r main \
  --build_references \
  --run_name build \
  --skip_genbank                            # <2>
```
1. `remote` is the name of your file in the `bio-raum/nf-configs` repository, without the `.config` extension.
2. The Genbank core database can be installed by omitting this line, note that this will take up several hundred Gb and may require a substantial amount of RAM to use. 


## Local

```bash
nextflow run bio-raum/FooDMe2 \
  -c $HOME/nextflow/local.config \          # <1>
  -r main \
  --build_references \
  --reference_base $HOME/nextflow/refs \    # <2>
  --run_name build \
  --skip_genbank                            # <3>
```
1. This should be the path to your local configuration file
2. This is the path to the folder to which the databses will be downloaded.
3. The Genbank core database can be installed by omitting this line, note that this will take up several hundred Gb and may require a substantial amount of RAM to use.

:::

The `$HOME/nextflow/refs` folder should now contain a folder called `foodme2` that itself contains several versioned databases.

### Testing the installation

When the database installation is finished, it is time for the last installation step before we dive in the analysis: testing the workflow.
Using the `test` profile as below will download a couple of sequencing data from ENA and run a standard workflow on them.

Don't worry about the parameters used here yet, we will go over it in a moment.

::: {.panel-tabset group="config"}

## Remote

```bash
nextflow run bio-raum/FooDMe2 \
  -profile remote,test \                    # <1>
  -r main \
  --run_name test
```
1. `remote` is the name of your file in the `bio-raum/nf-configs` repository, without the `.config` extension.

## Local

```bash
nextflow run bio-raum/FooDMe2 \
  -profile test
  -c $HOME/nextflow/local.config \          # <1>
  -r main \
  --reference_base $HOME/nextflow/refs \    # <2>
  --run_name test
```
1. This should be the path to your local configuration file
2. This is the path to the folder to which the databses will be downloaded.

:::

Nextflow should keep you informed of what is happening and after the workflow completed, you can check the `results` folder of
the current working directory to have a look at the results.

Congrats! We are done with the installation, and hopefully everything looks good.
The `results/reports` folder in particular contains the analysis result and useful QC metrics to evaluate your samples.
If you encountered an Error, check if you scuccesfully performed all the steps above.
Feel free to get in [touch for help troobleshooting](https://github.com/bio-raum/FooDMe2/issues/new?template=QUESTION.yml) or to [report a problem](https://github.com/bio-raum/FooDMe2/issues/new?template=BUG-REPORT.yml).

## Workflow management
The next steps will push further into real-life usage. But first let's take a short break to talk about a few important features of Nextflow.
In additon to running the workflow, Nextflow provides several utilities that are worth knowing (and using!).

### Clean temporary data

Each step of the workflow is executed in a folder generated and managed by Nextflow under the `work`folder in the current directory.
Nescessary files are **copied** to the `results` folder after workflow completion.
Although Nextflow relies a lot on symlinks, the `work` folder can quickly grow full of old an unesscessary data.

It is generally a <b>good practice</b> to keep separate folders for your various analyses so that you don't have to deal
with a balooning work directory. Still, at times you may want to clean up your pipeline runs:

- It is possible to simply delete the `work` folder manually using standard Unix commands.

```bash
rm -Rf work
```

- Nextflow also provides a [`clean`](https://www.nextflow.io/docs/latest/reference/cli.html#clean) utility that removes temporary data, 
which you can incorporate in you workflow (see examples at the end of this document).

```bash
nextflow clean
```

### Update workflows

Nextflow natively handles workflow versionning, to get the last version of the workflow run:

```bash
nextflow pull bio-raum/FooDMe2
```

### Versioning

FooDMe2 is semantically versioned, using a MAJOR.MINOR.PATCH model where:

- MAJOR is a major version, possibly introducing breaking changes
- MINOR is a minor version, introducing changes in workflows that are backwards compatible
- PATCH is mainly used for bugfix and documentation

Each new MAJOR or MINOR release requires a re-verification of the analytics. MAJOR changes can potentially require a re-validation.

Nextflow enables you you decide which version of the pipeline you use by providing a release tag to the `-r` argument:

```bash
nextflow run bio-raum/FooDMe2 \
  -r 1.1.0
```

To use the latest release use `-r main`, and to use the lastest development version use `-r dev`.

Note that while we do our best to release bug-free versions, the `dev` branch is under active development and may contain bugs or incomplete features. Use it at your own risk and report any issues you encounter.

## Running an analysis

In order to run the workflow on your own data we will have to modify the example above replacing the `test` profile with your data and provide an analysis method.
All options are listed in the [online documentation](https://bio-raum.github.io/FooDMe2/latest/user_doc/usage/#command-line-option).

There are two ways to provide data, assuming that the read files are saved under `/home/user/metabarcoding/rawdata/` (replace `user` with your user name):

- Provide a sample-sheet linking sample names to the files as a tab-separated table (recommended):

  ```{.sh filename="samples.tsv"}
  sample	fq1	fq2
  S1	~/home/user/metabarcoding/rawdata/S1_R1.fastq.gz	~/home/user/metabarcoding/rawdata/S1_R2.fastq.gz
  ```

  You can get the `create_sampleSheet.sh` script from the BfR Bio-informatics team here
  [{{<bi file-earmark-arrow-down >}}](https://gitlab.com/bfr_bioinformatics/AQUAMIS/-/raw/f517e42253224223493ea9ec41c9ecfe0cb65306/scripts/create_sampleSheet.sh?inline=false)
  ([doc](https://gitlab.com/bfr_bioinformatics/AQUAMIS/-/blob/f517e42253224223493ea9ec41c9ecfe0cb65306/scripts/create_sampleSheet.sh)) to automatically create sample-sheets.

- Provide a path wildcard linking both read files. Note that you cannot provide sample names and FooDMe2 will not be able to deal with multi-lane libraries this way.

  ```bash
  '/home/user/metabarcoding/rawdata/*_R{1,2}.fastq.gz'
  ```

  Where the `*` represent the variable part of the paths (usually the sample name, Illumina sample and Lane numbers) and `{1,2}` the numbering of the read pairs.
  Variables will not be expanded in the wildcards so you have to provide a **full path** and use **single quotes**.

For example:

:::: {.panel-tabset group="config"}
### Remote

::: {.panel-tabset group="input"}
#### Sample-sheet

```bash
nextflow run bio-raum/FooDMe2 \
  -profile remote \                                       # <1>
  -r 1.1.0 \                                              # <2>
  --run_name first_run \                                  # <3>
  --input $HOME/metabarcoding/rawdata/samples.tsv \       # <4>
  --primer_set amniotes_dobrovolny                        # <5>
```
1. `remote` is the name of your file in the `bio-raum/nf-configs` repository, without the `.config` extension.
2. Specifies the version of the FooDMe2 workflow to use. Here, we are using version 1.1.0.
3. Sets the name of the run to "first_run". This name will be used to label the output files.
5. Specifies the path to the sample-sheet containing the sample information.
6. Specifies the standard method to use for the analysis. List the available methods with `--list_primers`

#### Wildcard

```bash
nextflow run bio-raum/FooDMe2 \
  -profile remote \                                               # <1>
  -r 1.1.0 \                                                      # <2>
  --run_name first_run \                                          # <3>
  --reads '/home/user/metabarcoding/rawdata/*_R{1,2}.fastq.gz' \  # <4>
  --primer_set amniotes_dobrovolny                                # <5>
```
1. `remote` is the name of your file in the `bio-raum/nf-configs` repository, without the `.config` extension.
2. Specifies the version of the FooDMe2 workflow to use. Here, we are using version 1.1.0.
3. Sets the name of the run to "first_run". This name will be used to label the output files.
4. Specifies the path to the paired read-files using a wildcard. Note the use of full path and single quotes!
5. Specifies the standard method to use for the analysis. List the available methods with `--list_primers`

:::

### Local

::: {.panel-tabset group="input"}
#### Sample-sheet

```bash
nextflow run bio-raum/FooDMe2 \
  -c $HOME/nextflow/local.config \                        # <1>
  -r 1.1.0 \                                              # <2>
  --run_name first_run \                                  # <3>
  --input $HOME/metabarcoding/rawdata/samples.tsv \       # <4>
  --primer_set amniotes_dobrovolny \                      # <5>
  --reference_base $HOME/nextflow/refs                    # <6>
```
1. Path to your local configuration file.
2. Specifies the version of the FooDMe2 workflow to use. Here, we are using version 1.1.0.
3. Sets the name of the run to "first_run". This name will be used to label the output files.
4. Specifies the path to the paired read-files using a wildcard. Note the use of single quotes!
5. Specifies the standard method to use for the analysis. List the available methods with `--list_primers`
6. Path to the folder you used to install the references.

#### Wildcard

```bash
nextflow run bio-raum/FooDMe2 \
  -c $HOME/nextflow/local.config \                                # <1>
  -r 1.1.0 \                                                      # <2>
  --run_name first_run \                                          # <3>
  --reads '/home/user/metabarcoding/rawdata/*_R{1,2}.fastq.gz' \  # <4>
  --primer_set amniotes_dobrovolny \                              # <5>
  --reference_base $HOME/nextflow/refs                            # <6>
```
1. Path to your local configuration file.
2. Specifies the version of the FooDMe2 workflow to use. Here, we are using version 1.1.0.
3. Sets the name of the run to "first_run". This name will be used to label the output files.
4. Specifies the path to the paired read-files using a wildcard. Note the use of full path and single quotes!
5. Specifies the standard method to use for the analysis. List the available methods with `--list_primers`
6. Path to the folder you used to install the references.

:::

::::

You can additionaly use the `--output` parameter to specify a path for the results folder:

For example using the parameter below will write results to `$HOME/metabarcoding/results/first_run` instead of the `results` folder in the current directory.

```bash
--output $HOME/metabarcoding/results/first_run
```

## Going further

### Automating things

In a diagnostic laboratory setting, it is usefull automat things as much as possible to improve reproducibility and save time.
It particular, it is possible to automate workflow execution using simple scripts.

In the following paragraphs we will set up a small script that allows to run the workflow on data present locally in a folder.
Bear in mind that this is just a simple example that can be modified and expanded as will.

#### Folder structure

We already established an folder structure throughout this document. We will add a `scripts` folder in which we will save scripts, and an `archive` folder
to store rawdata after analysis.

It should look like this:

::: {.panel-tabset group="config"}
## Remote

```bash
$HOME
  ./nextflow
    ./refs                  # <1>
    ./envs                  # <2>
  ./metabarcoding
    ./rawdata
    ./results
    ./scripts
    ./archive
```
1. Should contain the `foodme2` folder in which the databases are saved
2. Should already contain plenty of environements, images or containers

## Local

```bash
$HOME
  ./nextflow
    ./refs                  # <1>
    ./envs                  # <2>
    ./local.config
  ./metabarcoding
    ./rawdata
    ./results
    ./scripts
    ./archive
```
1. Should contain the `foodme2` folder in which the databases are saved
2. Should already contain plenty of environements, images or containers

::::

The idea here, is that users can dump their sequencing files into the `rawdata` folder.
Running the script will ensure the data is analysed and output into the `results` folder and that the raw files are saved in the `archive` folder.

#### Creating the sample-sheet

For this example we will use the `create_sampleSheet.sh` script from the BfR bioinformatics team. Get it here 
[{{<bi file-earmark-arrow-down >}}](https://gitlab.com/bfr_bioinformatics/AQUAMIS/-/raw/f517e42253224223493ea9ec41c9ecfe0cb65306/scripts/create_sampleSheet.sh?inline=false)
([doc](https://gitlab.com/bfr_bioinformatics/AQUAMIS/-/blob/f517e42253224223493ea9ec41c9ecfe0cb65306/scripts/create_sampleSheet.sh))
if you don't have it already and save it to the `scripts` folder.

You can check how to use this scripts with:

```bash
bash $HOME/metabarcoding/scripts/create_sampleSheet.sh --help
```

Assuming standard Illumina file naming, you can generate a sample-sheet in the `rawdata` folder with:

```bash
bash $HOME/metabarcoding/scripts/create_sampleSheet.sh \
  -m illumina -f $HOME/metabarcoding/rawdata -o $HOME/metabarcoding/rawdata
```

#### Output and run naming

We can automate run naming by using the execution date.

```bash
run_id=$(date --iso-8601)                      # <1>
mkdir -p $HOME/metabarcoding/results/$run_id   # <2>
mkdir -p $HOME/metabarcoding/archive/$run_id   # <2>
```
1. Generate today's date as ISO8601 format 'YYYY-MM-DD'
2. Create output folders with the date as filenames. The `-p` options allows reuse of existing folders.

#### Putting it together

We can now put everything together in a neat little script:

::: {.panel-tabset group="config"}
## Remote

```{.sh filename="run_foodme2.sh"}
#!/usr/bin/env bash  # <1>
set -Eeuo pipefail  # <1>

VERSION=1.1.0  # <2>

# create samplesheet
bash $HOME/metabarcoding/scripts/create_sampleSheet.sh \
  -m illumina -f $HOME/metabarcoding/rawdata

# create output dirs
run_id=$(date --iso-8601)
mkdir -p $HOME/metabarcoding/results/$run_id
mkdir -p $HOME/metabarcoding/archive/$run_id

# run workflow
nextflow run bio-raum/FooDMe2 \
  -profile remote \  # <3>
  -r $VERSION \
  --run_name $run_id \
  --input $HOME/metabarcoding/rawdata/samples.tsv \
  --primer_set amniotes_dobrovolny \
  --outdir $HOME/metabarcoding/results/$run_id \
  && mv $HOME/metabarcoding/rawdata/* $HOME/metabarcoding/archive/$run_id \  # <4>
  && nextflow clean  # <5>
```
1. The first two lines are simply there to ensure that the correct shell is used and eventual errors are correctly reported.
2. Putting the version at the top of the file so it is easy to trace.
3. If nextflow finished succesfully, move all the content of the `rawdata` folder to a dated folder under `archive`.
4. THis should be the name of your remote configuration, without the `.config` extension.
5. If nextflow finished succesfully, clean the work folder and cache.

## Local


```{.sh filename="run_foodme2.sh"}
#!/usr/bin/env bash  # <1>
set -Eeuo pipefail  # <1>

VERSION='1.1.0'  # <2>

# create samplesheet
bash $HOME/metabarcoding/scripts/create_sampleSheet.sh \
  -m illumina -f $HOME/metabarcoding/rawdata

# create output dirs
run_id=$(date --iso-8601)
mkdir -p $HOME/metabarcoding/results/$run_id
mkdir -p $HOME/metabarcoding/archive/$run_id

# run workflow
nextflow run bio-raum/FooDMe2 \
  -c $HOME/nextflow/local.config \
  -r $VERSION \
  --run_name $run_id \
  --input $HOME/metabarcoding/rawdata/samples.tsv \
  --primer_set amniotes_dobrovolny \
  --reference_base $HOME/nextflow/refs \
  --outdir $HOME/metabarcoding/results/$run_id \
  && mv $HOME/metabarcoding/rawdata/* $HOME/metabarcoding/archive/$run_id \  # <3>
  && nextflow clean  # <4>
```
1. The first two lines are simply there to ensure that the correct shell is used and eventual errors are correctly reported.
2. Putting the version at the top of the file so it is easy to trace.
3. If nextflow finished succesfully, move all the content of the `rawdata` folder to a dated folder under `archive`.
4. If nextflow finished succesfully, clean the work folder and cache.

:::

Now make sure that the script is executable and run it:

```bash
chmod +x $HOME/metabarcoding/scripts/run_foodme2.sh
bash $HOME/metabarcoding/scripts/run_foodme2.sh
```

### Validation

Like we saw for the test, FooDMe2 packs a profile for quick validation of the mammals and birds methods.
Details of the execution and results can be found in the [online documentation](https://bio-raum.github.io/FooDMe2/latest/methods/amniotes_dobrovolny/).

The validation can be executed with:


::: {.panel-tabset group="config"}
## Remote

```bash
nextflow run bio-raum/FooDMe2 \
  -profile remote,dobrovolny_benchmark \
  -r $VERSION \
  --run_name validation \
  --outdir $HOME/metabarcoding/results/validation
```

## Local

```bash
nextflow run bio-raum/FooDMe2 \
  -profile dobrovolny_benchmark \
  -c $HOME/nextflow/local.config \
  -r $VERSION \
  --run_name validation \
  --reference_base $HOME/nextflow/refs \
  --outdir $HOME/metabarcoding/results/validation
```

:::

This will trigger the download of the dataset from the ENA and run FooDMe2 with the `amniotes_dobrovolny` primer sets configuration.
In addtion to normal runs, a confusion matrix will be produced, allowing to calculate metrics on precision and accuracy for the analysis.

It is possible that the connection with ENA breaks down, blocking the execution of the workflow. If restarting the command doesn√Ñt work, try to download the dataset
manually from ENA under project number [PRJEB57117](https://www.ebi.ac.uk/ena/browser/view/PRJEB57117).

You can then start the validation manually by providing the read files to the workflow as explained above, and additionally providing the ground truth table
supplied with the workflow. You can see the table [here](https://raw.githubusercontent.com/bio-raum/FooDMe2/main/assets/validation/dobrovolny_benchmark_groundtruth.csv).
Either save the content to a new file or use:

```bash
wget -O $HOME/metabarcoding/dobrovolny_benchmark_groundtruth.csv https://raw.githubusercontent.com/bio-raum/FooDMe2/main/assets/validation/dobrovolny_benchmark_groundtruth.csv
```

You can then provide the ground-truth table to the workflow with:

::: {.panel-tabset group="config"}
## Remote

```bash
nextflow run bio-raum/FooDMe2 \
  -profile remote \  # <3>
  -r $VERSION \
  --run_name validation \
  --input $HOME/metabarcoding/rawdata/samples.tsv \
  --primer_set amniotes_dobrovolny \
  --outdir $HOME/metabarcoding/results/validation \
  --ground_truth $HOME/metabarcoding/dobrovolny_benchmark_groundtruth.csv
```

## Local

```bash
nextflow run bio-raum/FooDMe2 \
  -c $HOME/nextflow/local.config \
  -r $VERSION \
  --run_name validation \
  --input $HOME/metabarcoding/rawdata/samples.tsv \
  --primer_set amniotes_dobrovolny \
  --reference_base $HOME/nextflow/refs \
  --outdir $HOME/metabarcoding/results/validation \
  --ground_truth $HOME/metabarcoding/dobrovolny_benchmark_groundtruth.csv
```

:::


You can also provide you own truth table to perform validation using your own samples. To know how check the "Benchmarking" part of the [Usage documentation](https://bio-raum.github.io/FooDMe2/latest/user_doc/usage/#benchmarking).

### Non-standard methods

FooDMe2 is of course not limited to the birds and mammals method. You can analyze any kind of metabarcoding data, this will require that you modify
several analysis parameters. Which parameters can be accessed and what they do is described in the
[documentation](https://bio-raum.github.io/FooDMe2/latest/user_doc/usage/#advanced-options).
